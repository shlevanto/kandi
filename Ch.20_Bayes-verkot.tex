\chapter{Bayes-verkot\label{bayes}}

\section{Graafiset mallit}
Graafinen malli (\emph{graphical model}) yhdistelee tilastotiedettä, todennäköisyysteoriaa ja graafiteoriaa, ja kuvaa näiden avulla satunnaismuuttujia ja näiden välisiä yhteyksiä graafeina \citep{jordan_graphical_2004}. Sinällään graafisen mallin käsite ei ole kovin tarkkarajainen, vaan sillä voidaan tarkoittaa aineiston kuvaamista graafimuodossa myös ilman, että graafin muodostamiseen on käytetty laskentaa. \citet{ramsahai_connecting_2020} jaottelee graafiset mallit ilman laskentaa muodostettuihin kvalitatiivisiin malleihin, ja laskennan avulla muodostettuihin kvantitatiivisiin malleihin. 

\citet{jordan_graphical_2004} puolestaan määrittelee probabilistisen graafisen mallin sellaiseksi, jossa graafin solmut kuvaavat satunnaismuuttujia ja kaaret näiden välisiä riippuvuuksia. Tällaista graafista mallia kutsutaan \emph{Bayes-verkoksi} (\texttt{Bayesian network}), kun mallissa käytetään suunnattua syklitöntä graafia ja vastaavasti Markov-verkoksi, jos kaaret ovat suuntaamattomia \citep{ruggeri_bayesian_2008}. Ensimmäisenä Bayes-verkkojen käyttömahdollisuuksia on esittänyt \citet{pearl_fusion_1986}.

\section{Bayes-verkko}
Bayes-verkko on \emph{suunnattu syklitön verkko} (\texttt{Directed Acyclic Graph, DAG}), joka kuvaa satunnaismuuttujien yhteistodennäköisyysjakaumaa \citep{ruggeri_bayesian_2008}. Määritelmällisesti Bayes-verkko on pari 
$$
    B = \langle G, \Theta \rangle
$$
jossa 
$$
    G = \{V, E\}, V = \{X_1, X_2,\ldots X_n\}
$$ 
eli graafin solmut kuvaavat satunnaismuuttujia ja kaarien joukko $E$ kuvaa näiden välisiä suoria riippuvuuksia. Bayes-verkossa kukin muuttuja on riippumaton kaikista niistä muuttujista, jotka eivät ole tämän muuttujan jälkeläisiä kun tälle muuttujalle on määritelty verkossa sen edeltäjien muodostama joukko. Parametrijoukko $\Theta$ koostuu satunnaismuuttujien $X_i$ parametreista 
$$
    \theta_{x_i|\pi_i} = P_B(x_i|\pi_i)
$$ 
eli ehdollisista todennäköisyyksistä tapaukselle $x_i$ ehtonaan muuttujan $X_i$ edeltäjien joukko verkossa $G$ \citep{ruggeri_bayesian_2008, myllymaki_bayes-verkkojen_1998}. Joukon $\Theta$ alkiot voidaan esittää \emph{ehdollisten todennäköisyyksien taulukoina}  (\texttt{conditional probability table, CPT}) kaikille niille tapauksille, joita muuttujan edeltäjiensä ehdoilla voi saada \citep{zhang_brief_2019}.

Tarkastellaan kuvassa \ref{fig:bayes-esim} esitettyä yksinkertaista Bayes-verkkoa. Kesällä on varsin suuri todennäköisyys, että henkilö on hiljattain käynyt uimassa $P(U) = 0.8$. Toiveikkaasti ajatellen todennäköisyys että kesäpäivänä sataa on melko pieni $P(S) = 0.1$. Ajatellaan myös, että uiminen ei ole riippuvaista sateesta eli sateella on yhtä todennäköistä käydä uimassa kuin poutasäässä.

\begin{center}
    \captionof{figure}{Kesäinen Bayes-verkko \label{fig:bayes-esim}}
    \tikzfig{bayes_ex_1}
\end{center}
    
Jos henkilö käy uimassa, on hänen tukkansa märkä, mutta paita kuiva. Jos sataa, sekä tukka että paita kastuvat. Tukan märkyys riippuu siis sekä sateesta että uinnista. Vastaavasti paidan märkyys riippuu vain sateesta. Taulukossa \ref{table:bayes-esim} on esitetty tämän esimerkkiverkon parametrijoukon alkiot.

\begin{center}

\captionof{table}{Esimerkkiverkon parametrijoukko $\Theta$ \label{table:bayes-esim}} 
\begin{tabular}{ cccc }   % top level tables, with 4 columns
    \begin{tabular}{||c ||} 
     \hline
     S \\
     \hline\hline
     $s^1 = 0.1$ \\ 
     $s^0 = 0.9$ \\
     \hline
    \end{tabular} &  
    
    % table 2
    \begin{tabular}{||c ||} 
     \hline
     U \\
     \hline\hline
     $u^1 = 0.8$ \\ 
     $u^0 = 0.2$ \\
     \hline
    \end{tabular} &
    
    % table 3
    \begin{tabular}{||c ||} 
     \hline
     P \\
     \hline\hline
     $p^1 | s^1 = 1$ \\ 
     $p^1 | s^0 = 0$ \\
     $p^0 | s^0 = 1$ \\
     $p^0 | s^1 = 0$ \\
     \hline
    \end{tabular} &
    
    % table 3
    \begin{tabular}{||c ||} 
     \hline
     T \\
     \hline\hline
     $t^1 | (s^1 \cap u^1) = 1$ \\ 
     $t^1 | (s^1 \cap u^0) = 1$ \\
     $t^0 | (s^0 \cap u^1) = 1$ \\
     $t^0 | (s^0 \cap u^0) = 0$\\
     \hline
    \end{tabular}
\end{tabular}
\end{center}
Mallin esittämän yhteistodennäköisyysjakauman avulla voimme laskea todennäköisyyden esimerkiksi sille, että satunnaisena kesäpäivänä henkilöllä on märkä tukka
$$
    P(T) = (0.1\cdot0.8) + (0.1\cdot0.2) + (0.9\cdot0.8) = 0.82
$$

Bayes-verkon avulla voimme nyt paitsi hahmottaa, miten eri muuttujat ovat riippuvaisia toisistaan, myös laskea Bayesin kaavaa \ref{eq:bayes} käyttäen prioritodennäköisyyksiä esim. todennäköisyys sille, että sataa kun tiedetään että henkilön tukka on märkä.

$$
  P(S|T) = \frac{P(T|S)P(S)}{P(T)} = \frac{1 \cdot P(0.1)}{P(0.82)} = 0.12   
$$

Bayes-verkko kuvaa siis satunnaismuuttujajoukon yhteistodennäköisyysjakaumaa ja siihen kuuluvien muuttujien välisiä riippuvuussuhteita. Vaikka esimerkissä \ref{fig:bayes-esim} on selkeästi kyse kausaalisuhteista ja Bayes-verkossa riippuvuuksia kuvataan suunnattuina kaarina, painottavat mm. \citet{ruggeri_bayesian_2008, myllymaki_bayes-verkkojen_1998} että Bayes-verkon kaaret suuntineen eivät välttämättä kuvaa muuttujien välisiä kausaalisuhteita. Kausaalisuhteiden päätteleminen tai oppiminen datasta on haastavaa, mutta joissain tapauksissa mahdollista ja muodostaa oman mielenkiintoisen aihekokonaisuutensa. \citet{vowels_dya_2022} tarkastelee tuoreessa katsauksessaan tähän soveltuvia menetelmiä, mutta niiden käsitteleminen rajautuu tämän opinnäytetyön ulkopuolelle.

\section{Bayes-verkkojen pisteyttäminen}

Bayes-verkkojen käytännön sovelluksissa verkon rakenne on usein tuntematon \citep{ruggeri_bayesian_2008}, eli muuttujien välisiä riippuvuuksia ei tiedetä. Tällöin mielenkiintoiseksi tehtäväksi muodostuu verkon rakenteen päätteleminen tai laskeminen datan pohjalta. \citet{myllymaki_bayes-verkkojen_1998} mukaan mallintamisessa edetään mallin sopivuuden arvioimisesta verkon rakenteen oppimisen kautta parametrijoukon laskemiseen. 

Bayes-verkon \emph{kokonaisuskottavuutta} eli sopivuutta \citep{myllymaki_bayes-verkkojen_1998} tulkitaan niin, että todennäköisin malli annetulla datajoukolla on sopivin. Kokonaisuskottavuutta kuvataan ehdollisena posterioritodennäköisyytenä $P(M | \mathcal{D})$, missä $\mathcal{D}$ on mallin opettamiseen käytetty aineisto. Tunnetuin Bayes-verkon uskottavuutta kuvaava pisteytys on \citet{schwarz_estimating_1978} esittelemä \emph{BIC} (\texttt{Bayesian Information Criterion}) ja sitä vastaava \emph{MDL} (\texttt{Minimum Description Length}) \citep{ruggeri_bayesian_2008, liu_empirical_2012}. \citet{myllymaki_bayes-verkkojen_1998} esittävät BIC:n approksimaatioon kaavan

$$
P(\mathcal{D}|M) = \frac{P(\mathcal{D}|M, \bar{\theta})}{N{\frac{d(M)}{2}}}
$$

jossa $\bar{\theta}$ kuvaa suurimman uskottavuuden parametrijoukkoa, $d(M)$ malliparametrien lukumäärä ja $N$ mallin opettamiseen käytetyn datajoukon koko. MDL puolestaan on tämän approksimaation negatiivinen logaritmi (kts. \citet{rissanen_stochastic_1987}).

BIC:n ohella toinen suosittu pisteytys on \emph{BDeu} (\texttt{Bayesian Dirichlet equivalent uniform})  \citep{scanagatta_survey_2019}. Lisäksi on olemassa erilaisia paikallisen rakenteen optimointiin perustuvia pisteytyksiä. 

\section{Bayes-verkon rakenteen oppiminen}

Bayes-verkon rakenteen oppiminen on laskennallisesti vaativa, todetusti NP-kova ongelma \citep{chickering_large-sample_2004}. On helppo nähdä, että jo yksinkertaisten dikotomisten muuttujien muodostamassa Bayes-verkossa on $2^v$ mahdollista rakennetta, kun $v$ on satunnaismuuttujien määrä ja muuttujien välillä on riippuvuuksia. Nähdään, että kaikkien mahdollisten verkkojen laskeminen johtaa pahimmassa tapauksessa eksponentiaalisesti kasvavaan joukkoon. Rakenteen oppimisen laskennallinen vaativuus tekeekin siitä erityisen mielenkiintoisen ongelman, jonka ratkaiseminen vaatii sopivaa tasapainottelua tarkkuuden ja laskentatehovaatimusten välillä.

Rakenteen oppimista voidaan lähestyä kahdella tavalla: \emph{pisteyttämällä} (\texttt{score based}),  \emph{rajoitepohjaisesti} (\texttt{constraint based}) \citep{ramsahai_connecting_2020, scutari_learning_2010} tai yhdistämällä näitä esim. \citet{li_hybrid_2018}. Bayes-verkon rakennetta voidaan myös arvioida ja täydentää kvalitatiivisesti käyttämällä asiantuntijatietoa muuttujien välisistä suhteista \citep{ruggeri_bayesian_2008, myllymaki_bayes-verkkojen_1998}.

\subsection{Pisteyttämiseen perustuvat algoritmit}

Pisteyttämiseen pohjautuvissa algoritmeissa kokeillaan ertilaisia vaihtoehtoisia rakenteita, ja valitaan niistä sellainen Bayes-verkko, jonka sopivuutta kuvaava funktio (esim. BIC) saa parhaan mahdollisen arvon. Empiirisellä datalla tehdyssä vertailussa \citet{liu_empirical_2012} ovat osoittaneet, että nimenomaan BIC ja sitä vastaava MDL antavat hyvin luotettavia tuloksia verkon rakenteen oppimisessa.  

\subsection{Rajoitepohjaiset algoritmit}

Rajoitepohjaisessa lähestymistavassa puolestaan solmuissa oleville muuttujille ajetaan tilastollisia testejä (esim. $\chi^2$), joiden avulla arvioidaan muuttujien riippumattomuutta ja näin pyritään löytämään aineistoon sopivin eli uskottavin verkko \citep{ramsahai_connecting_2020, scutari_learning_2010}. <avaa tätä>

 
\citet{scanagatta_survey_2019} tarkastelevat tarkemmin erilaisia menetelmiä rakenteen oppimiseen. He jakavat ongelman ensinnäkin kahteen osaan: \emph{edeltäjäjoukon tunnistaminen} (\texttt{parent set identification}) ja \emph{verkon rakenteen optimointi} (\texttt{structure optimization}). 

\citet{scanagatta_survey_2019} toteavat, että edeltäjäjoukon tunnistamisessa naiivi ratkaisu on laskea kaikki mahdolliset Bayes-verkot ja sovittaa niitä. Tällöin aikavaativuudeksi saadaan $O(n^k)$, jossa $n$ on solmujen eli satunnaismuuttujien määrä ja $k$ on yläraja sille, montako vanhempaa solmulla voi olla. Katsauksessa esitellään myös muutamia vaihtoehtoja, joissa muodostettavien Bayes-verkkojen määrää tai niiden hakua rajoittamalla saadaan suhteellisen tehokkaita laskentamentelmiä. Joko luvulle $k$ voidaan asettaa rajoite tai sitten rajoitetta voidaan kiertää esim. käyttämällä ahnetta hakua tai approksimointimenetelmiä.

\citet{scanagatta_survey_2019} mukaan Bayes-verkon rakenteen oppimisen algoritmit keskittyvät useimmiten rakenteen optimoimiseen. Tätä ongelmaa voidaan edelleen lähestyä kahdella eri tavalla:  rajoittamalla tuloksena saatavan Bayes-verkon puuleveyttä tai optimoimalla rakennetta yleisesti eli ilman rajoitteita tuloksena saatavalle Bayes-verkolle. Puuleveyden rajaamiseen perustuvat menetelmät puolestaan perustuvat kaarien heuristiseen lisäämiseen, dynaamiseen ohjelmointiin, kokonaislukuohjelmointiin tai puuleveydeltään $k$ olevien maksimaalisten graafien eli $k$-puiden otoksiin. Yleisen optimoinnin algoritmeille on esitetty useita eri lähestymistapoja: dynaaminen ohjelmointi, lyhimmän polun löytämisen soveltaminen ja branch and bound. Parhaita tuloksia näytettäisiin saavan algoritmeilla, jotka hyödyntävät \emph{kokonaislukujen lineaarista ohjelmointia} (\texttt{integer linear programming, ILP}). \citet{bartlett_integer_2017} käsittelevät artikkelissaan tarkemmin  ILP:n käyttömahdollisuuksia.

\citet{mittal_review_2011} esittelevät yhteenvedon ja vertailun muutamasta yleisimmin käytetystä algoritmista Bayes-verkon rakenteen oppimiseksi. Lisäksi he vertailevat algoritmien tarkkuutta ja tehokkuutta erikokoisissa aineistoissa. Käyn seuraavaksi läpi muutamia keskeisiä pisteyttämiseen ja rajoitepohjaisuuteen perustuvia algoritmeja.

