
@article{ramsahai_connecting_2020,
	title = {Connecting actuarial judgment to probabilistic learning techniques with graph theory},
	url = {http://arxiv.org/abs/2007.15475},
	doi = {10.48550/arXiv.2007.15475},
	abstract = {Graphical models have been widely used in applications ranging from medical expert systems to natural language processing. Their popularity partly arises since they are intuitive representations of complex inter-dependencies among variables with efficient algorithms for performing computationally intensive inference in high-dimensional models. It is argued that the formalism is very useful for applications in the modelling of non-life insurance claims data. It is also shown that actuarial models in current practice can be expressed graphically to exploit the advantages of the approach. More general models are proposed within the framework to demonstrate the potential use of graphical models for probabilistic learning with telematics and other dynamic actuarial data. The discussion also demonstrates throughout that the intuitive nature of the models allows the inclusion of qualitative knowledge or actuarial judgment in analyses.},
	journaltitle = {{arXiv}:2007.15475 [cs, q-fin, stat]},
	author = {Ramsahai, Roland R.},
	urldate = {2021-10-25},
	date = {2020-07-29},
	eprinttype = {arxiv},
	eprint = {2007.15475},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Finance - Statistical Finance, Statistics - Applications},
	file = {arXiv Fulltext PDF:C\:\\Users\\LEVANSI\\Zotero\\storage\\PLH2BMB4\\Ramsahai - 2020 - Connecting actuarial judgment to probabilistic lea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\LEVANSI\\Zotero\\storage\\4ZF3Q7TG\\2007.html:text/html},
}

@book{myllymaki_bayes-verkkojen_1998,
	location = {Helsinki},
	title = {Bayes-verkkojen mahdollisuudet},
	volume = {58},
	isbn = {951-53-1391-0},
	publisher = {Tekes},
	author = {Myllymäki, Petri and Tirri, Henry},
	date = {1998},
	file = {bvmahd.pdf:C\:\\Users\\LEVANSI\\Zotero\\storage\\YZ27W5DV\\bvmahd.pdf:application/pdf},
}

@article{jordan_graphical_2004,
	title = {Graphical Models},
	volume = {19},
	issn = {0883-4237},
	url = {https://www.jstor.org/stable/4144379},
	doi = {10.1214/088342304000000026},
	abstract = {Statistical applications in fields such as bioinformatics, information retrieval, speech processing, image processing and communications often involve large-scale models in which thousands or millions of random variables are linked in complex ways. Graphical models provide a general methodology for approaching these problems, and indeed many of the models developed by researchers in these applied fields are instances of the general graphical model formalism. We review some of the basic ideas underlying graphical models, including the algorithmic ideas that allow graphical models to be deployed in large-scale data analysis problems. We also present examples of graphical models in bioinformatics, error-control coding and language processing.},
	pages = {140--155},
	number = {1},
	journaltitle = {Statistical Science},
	author = {Jordan, Michael I.},
	urldate = {2022-09-14},
	date = {2004},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {JSTOR Full Text PDF:C\:\\Users\\LEVANSI\\Zotero\\storage\\KMF4INY3\\Jordan - 2004 - Graphical Models.pdf:application/pdf},
}

@article{scanagatta_survey_2019,
	title = {A survey on Bayesian network structure learning from data},
	volume = {8},
	issn = {2192-6360},
	url = {https://doi.org/10.1007/s13748-019-00194-y},
	doi = {10.1007/s13748-019-00194-y},
	abstract = {A necessary step in the development of artificial intelligence is to enable a machine to represent how the world works, building an internal structure from data. This structure should hold a good trade-off between expressive power and querying efficiency. Bayesian networks have proven to be an effective and versatile tool for the task at hand. They have been applied to modeling knowledge in a variety of fields, ranging from bioinformatics to law, from image processing to economic risk analysis. A crucial aspect is learning the dependency graph of a Bayesian network from data. This task, called structure learning, is {NP}-hard and is the subject of intense, cutting-edge research. In short, it can be thought of as choosing one graph over the many candidates, grounding our reasoning over a collection of samples of the distribution generating the data. The number of possible graphs increases very quickly at the increase in the number of variables. Searching in this space, and selecting a graph over the others, becomes quickly burdensome. In this survey, we review the most relevant structure learning algorithms that have been proposed in the literature. We classify them according to the approach they follow for solving the problem and we also show alternatives for handling missing data and continuous variable. An extensive review of existing software tools is also given.},
	pages = {425--439},
	number = {4},
	journaltitle = {Progress in Artificial Intelligence},
	shortjournal = {Prog Artif Intell},
	author = {Scanagatta, Mauro and Salmerón, Antonio and Stella, Fabio},
	urldate = {2022-09-15},
	date = {2019-12-01},
	langid = {english},
	keywords = {Machine learning, Statistics, Bayesian network, Structure learning},
	file = {Full Text PDF:C\:\\Users\\LEVANSI\\Zotero\\storage\\2424TA3A\\Scanagatta et al. - 2019 - A survey on Bayesian network structure learning fr.pdf:application/pdf},
}

@inproceedings{mittal_review_2011,
	title = {A review of some Bayesian Belief Network structure learning algorithms},
	doi = {10.1109/ICICS.2011.6173579},
	abstract = {Bayesian Belief Networks ({BBNs}) are useful in modeling complex situations. Such graphical models help in giving better insight and understanding of the situation. Many algorithms for machine learning of {BBN} structures have been developed. In this paper six different algorithms have been reviewed by constructing {BBN} structures for two different datasets using various algorithms. Some inferences have been drawn from the results obtained from the study which may help in decision making.},
	eventtitle = {2011 8th International Conference on Information, Communications \& Signal Processing},
	pages = {1--5},
	booktitle = {2011 8th International Conference on Information, Communications \& Signal Processing},
	author = {Mittal, Sangeeta and Maskara, S. L.},
	date = {2011-12},
	keywords = {Diseases, Bayesian Belief Network, Bayesian methods, Hepatitis Domain Diseases, Inference algorithms, Learning systems, Machine learning algorithms, Size measurement, Structure Learning, Urinary System Diseases},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\LEVANSI\\Zotero\\storage\\2QDS7UP7\\6173579.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\LEVANSI\\Zotero\\storage\\ZPI6UWJY\\Mittal and Maskara - 2011 - A review of some Bayesian Belief Network structure.pdf:application/pdf},
}

@article{malone_empirical_2018,
	title = {Empirical hardness of finding optimal Bayesian network structures: algorithm selection and runtime prediction},
	volume = {107},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-017-5680-2},
	doi = {10.1007/s10994-017-5680-2},
	shorttitle = {Empirical hardness of finding optimal Bayesian network structures},
	abstract = {Various algorithms have been proposed for finding a Bayesian network structure that is guaranteed to maximize a given scoring function. Implementations of state-of-the-art algorithms, solvers, for this Bayesian network structure learning problem rely on adaptive search strategies, such as branch-and-bound and integer linear programming techniques. Thus, the time requirements of the solvers are not well characterized by simple functions of the instance size. Furthermore, no single solver dominates the others in speed. Given a problem instance, it is thus a priori unclear which solver will perform best and how fast it will solve the instance. We show that for a given solver the hardness of a problem instance can be efficiently predicted based on a collection of non-trivial features which go beyond the basic parameters of instance size. Specifically, we train and test statistical models on empirical data, based on the largest evaluation of state-of-the-art exact solvers to date. We demonstrate that we can predict the runtimes to a reasonable degree of accuracy. These predictions enable effective selection of solvers that perform well in terms of runtimes on a particular instance. Thus, this work contributes a highly efficient portfolio solver that makes use of several individual solvers.},
	pages = {247--283},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Malone, Brandon and Kangas, Kustaa and Järvisalo, Matti and Koivisto, Mikko and Myllymäki, Petri},
	urldate = {2022-09-19},
	date = {2018-01-01},
	langid = {english},
	keywords = {Bayesian networks, Structure learning, Algorithm portfolio, Algorithm selection, Empirical hardness, Hyperparameter optimization, Runtime prediction},
}

@article{chickering_large-sample_2004,
	title = {Large-Sample Learning of Bayesian Networks is {NP}-Hard},
	volume = {5},
	issn = {1532-4435},
	abstract = {In this paper, we provide new complexity results for algorithms that learn discrete-variable Bayesian networks from data. Our results apply whenever the learning algorithm uses a scoring criterion that favors the simplest structure for which the model is able to represent the generative distribution exactly. Our results therefore hold whenever the learning algorithm uses a consistent scoring criterion and is applied to a sufficiently large dataset. We show that identifying high-scoring structures is {NP}-hard, even when any combination of one or more of the following hold: the generative distribution is perfect with respect to some {DAG} containing hidden variables; we are given an independence oracle; we are given an inference oracle; we are given an information oracle; we restrict potential solutions to structures in which each node has at most k parents, for all k{\textgreater}=3.Our proof relies on a new technical result that we establish in the appendices. In particular, we provide a method for constructing the local distributions in a Bayesian network such that the resulting joint distribution is provably perfect with respect to the structure of the network.},
	pages = {1287--1330},
	journaltitle = {The Journal of Machine Learning Research},
	shortjournal = {J. Mach. Learn. Res.},
	author = {Chickering, David Maxwell and Heckerman, David and Meek, Christopher},
	date = {2004-12-01},
}

@incollection{ruggeri_bayesian_2008,
	location = {Chichester, {UK}},
	title = {Bayesian Networks},
	isbn = {978-0-470-01861-3 978-0-470-06157-2},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9780470061572.eqr089},
	pages = {eqr089},
	booktitle = {Encyclopedia of Statistics in Quality and Reliability},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Ben-Gal, Irad},
	editor = {Ruggeri, Fabrizio and Kenett, Ron S. and Faltin, Frederick W.},
	urldate = {2022-09-19},
	date = {2008-03-15},
	langid = {english},
	doi = {10.1002/9780470061572.eqr089},
	file = {BN.pdf:C\:\\Users\\LEVANSI\\Zotero\\storage\\MJTU3WIG\\BN.pdf:application/pdf},
}

@article{scutari_learning_2010,
	title = {Learning Bayesian networks with the bnlearn R package},
	volume = {35},
	pages = {1--22},
	number = {3},
	journaltitle = {Journal of Statistical Software},
	author = {Scutari, Marco},
	date = {2010},
	file = {0908.3817.pdf:C\:\\Users\\LEVANSI\\Zotero\\storage\\G9V6RDQ9\\0908.3817.pdf:application/pdf},
}

@book{diestel_graph_2017,
	location = {New York, {NY}},
	title = {Graph theory},
	isbn = {978-3-662-53621-6},
	publisher = {Springer Berlin Heidelberg},
	author = {Diestel, Reinhard},
	date = {2017},
}

@book{dasgupta_probability_2011,
	location = {New York},
	title = {Probability for statistics and machine learning: fundamentals and advanced topics},
	isbn = {9781441996336 9781441996343},
	series = {Springer texts in statistics},
	shorttitle = {Probability for statistics and machine learning},
	pagetotal = {782},
	publisher = {Springer},
	author = {{DasGupta}, Anirban},
	date = {2011},
	note = {{OCLC}: ocn706920643},
	keywords = {Machine learning, Mathematical statistics, Probabilities, Stochastic processes},
}


@article{karolaakso_socioeconomic_2020,
	title = {Socioeconomic factors in disability retirement due to mental disorders in Finland},
	url = {https://academic.oup.com/eurpub/advance-article/doi/10.1093/eurpub/ckaa132/5905677},
	doi = {10.1093/eurpub/ckaa132},
	abstract = {{AbstractBackground}.  Previous research has identified low socioeconomic status ({SES}) as an epidemiological risk factor for early retirement and disability pensi},
	journaltitle = {European Journal of Public Health},
	shortjournal = {Eur J Public Health},
	author = {Karolaakso, Tino and Autio, Reija and Näppilä, Turkka and Nurmela, Kirsti and Pirkola, Sami},
	urldate = {2020-09-16},
	date = {2020},
	langid = {english},
	keywords = {luettu},
}

@article{karolaakso_contextual_2021,
	title = {Contextual and mental health service factors in mental disorder-based disability pensioning in Finland – a regional comparison},
	volume = {21},
	issn = {1472-6963},
	url = {https://doi.org/10.1186/s12913-021-07099-4},
	doi = {10.1186/s12913-021-07099-4},
	abstract = {We investigated the regional differences in all mental disorder disability pensions ({DP}) between 2010 and 2015 in Finland, and separately in mood disorders and non-affective psychotic disorder {DP}. We also studied the contribution of several district-level contextual and mental health service factors to mental disorder {DP}.},
	pages = {1081},
	number = {1},
	journaltitle = {{BMC} Health Services Research},
	shortjournal = {{BMC} Health Services Research},
	author = {Karolaakso, Tino and Autio, Reija and Näppilä, Turkka and Leppänen, Helena and Rissanen, Päivi and Tuomisto, Martti T. and Karvonen, Sakari and Pirkola, Sami},
	urldate = {2021-11-05},
	date = {2021-10-11},
	keywords = {Compositional factor, Contextual factor, Disability pension, Mental disorders, Mental health services, Regional differences},
}

@software{bayesnetsjl_2021,
	title = {{BayesNets}.jl},
	url = {https://github.com/sisl/BayesNets.jl},
	publisher = {Stanford Intelligent Systems Laboratory},
	date = {2021-05-03},
}

@article{pearl1986fusion,
  title={Fusion, propagation, and structuring in belief networks, Artificial Intelligence, vol. 29},
  author={Pearl, J},
  year={1986},
  publisher={P 24-1-288}
}

@inproceedings{zhang_brief_2019,
	title = {A brief review of Bayesian belief network},
	doi = {10.1109/CCDC.2019.8832649},
	eventtitle = {2019 Chinese Control And Decision Conference ({CCDC})},
	pages = {3910--3914},
	booktitle = {2019 Chinese Control And Decision Conference ({CCDC})},
	author = {Zhang, Jinqing and Yue, Haosong and Wu, Xingming and Chen, Weihai},
	date = {2019-06},
	note = {{ISSN}: 1948-9447},
	keywords = {Bayes methods, Bayesian belief network, Fault diagnosis, Peer-to-peer computing, Probabilistic logic, Training, fault diagnosis, reliability analysis, structure learning},
}


@article{kaur_review_2013,
	title = {A Review of Machine Learning based Anomaly Detection Techniques},
	volume = {2},
	issn = {23198656},
	url = {http://www.ijcat.com/archives/volume2/issue2/ijcatr02021020.pdf},
	doi = {10.7753/IJCATR0202.1020},
	pages = {185--187},
	number = {2},
	journaltitle = {International Journal of Computer Applications Technology and Research},
	shortjournal = {{IJCATR}},
	author = {Kaur, Harjinder and Singh, Gurpreet and Minhas, Jaspreet},
	urldate = {2022-10-03},
	date = {2013-03-10},
	file = {Full Text:/home/levantsi/Zotero/storage/MWXKWKP9/Kaur et al. - 2013 - A Review of Machine Learning based Anomaly Detecti.pdf:application/pdf},
}


@article{vowels_dya_2022,
	title = {D’ya Like {DAGs}? A Survey on Structure Learning and Causal Discovery},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3527154},
	doi = {10.1145/3527154},
	shorttitle = {D’ya Like {DAGs}?},
	abstract = {Causal reasoning is a crucial part of science and human intelligence. In order to discover causal relationships from data, we need structure discovery methods. We provide a review of background theory and a survey of methods for structure discovery. We primarily focus on modern, continuous optimization methods, and provide reference to further resources such as benchmark datasets and software packages. Finally, we discuss the assumptive leap required to take us from structure to causality.},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Vowels, Matthew J. and Camgoz, Necati Cihan and Bowden, Richard},
	urldate = {2022-09-23},
	date = {2022-03-14},
	note = {Just Accepted},
	file = {Full Text PDF:/home/levantsi/Zotero/storage/NYC7L4DJ/Vowels et al. - 2022 - D’ya Like DAGs A Survey on Structure Learning and.pdf:application/pdf},
}

@article{behjati_improved_2020,
	title = {Improved K2 algorithm for Bayesian network structure learning},
	volume = {91},
	issn = {0952-1976},
	url = {https://www.sciencedirect.com/science/article/pii/S095219762030083X},
	doi = {10.1016/j.engappai.2020.103617},
	abstract = {In this paper, we study the problem of learning the structure of Bayesian networks from data, which takes a dataset and outputs a directed acyclic graph. This problem is known to be {NP}-hard. Almost most of the existing algorithms for structure learning can be classified into three categories: constraint-based, score-based, and hybrid methods. The K2 algorithm, as a score-based algorithm, takes a random order of variables as input and its efficiency is strongly dependent on this ordering. Incorrect order of variables can lead to learning an incorrect structure. Therefore, the main challenge of this algorithm is strongly dependency of output quality on the initial order of variables. The main contribution of this paper is to derive a significant order of variables from the given dataset. Also, one of the significant challenges of structure learning is to find a practical structure learning approach to learn an optimal structure from complex and high-dimensional datasets in a reasonable time. We propose a new fast and straightforward algorithm for addressing this problem in a reasonable time. The proposed algorithm is based on an ordering by extracting strongly connected components of the graph built from data. We reduce the super-exponential search space of structures to the smaller space of nodes ordering. We evaluated the proposed algorithm using some standard benchmark datasets and compare the results with the results obtained from some state of the art algorithms. Finally, we show that the proposed algorithm is competitive with some algorithms for structure learning.},
	pages = {103617},
	journaltitle = {Engineering Applications of Artificial Intelligence},
	shortjournal = {Engineering Applications of Artificial Intelligence},
	author = {Behjati, Shahab and Beigy, Hamid},
	urldate = {2022-10-10},
	date = {2020-05-01},
	langid = {english},
	keywords = {Bayesian network, Constraint-based algorithm, Score-based algorithms, Structure learning},
	file = {ScienceDirect Full Text PDF:/home/levantsi/Zotero/storage/55CPRZJ9/Behjati and Beigy - 2020 - Improved K2 algorithm for Bayesian network structu.pdf:application/pdf;ScienceDirect Snapshot:/home/levantsi/Zotero/storage/BSKCJ6BB/S095219762030083X.html:text/html},
}
@article{atienza_pybnesian_2022,
	title = {{PyBNesian}: An extensible python package for Bayesian networks},
	volume = {504},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231222008438},
	doi = {10.1016/j.neucom.2022.06.112},
	shorttitle = {{PyBNesian}},
	abstract = {Bayesian networks are probabilistic graphical models that are commonly used to represent the uncertainty in data. The {PyBNesian} package provides an implementation for many different types of Bayesian network models and some variants, such as conditional Bayesian networks and dynamic Bayesian networks. In addition, the package can be easily extended with new components that can interoperate with those already implemented. Furthermore, the package also implements other related models such as kernel density estimation using {OpenCL} 1.2+ to enable {GPU} acceleration. {PyBNesian} is totally free and open-source under the {MIT} license.},
	pages = {204--209},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Atienza, David and Bielza, Concha and Larrañaga, Pedro},
	urldate = {2022-09-23},
	date = {2022-09-14},
	langid = {english},
	keywords = {Bayesian networks, Conditional independence, Dynamic models, Kernel density estimation},
	file = {Full Text:/home/levantsi/Zotero/storage/UFK8EJ73/Atienza et al. - 2022 - PyBNesian An extensible python package for Bayesi.pdf:application/pdf;ScienceDirect Snapshot:/home/levantsi/Zotero/storage/RTA9KCB6/S0925231222008438.html:text/html},
}


@article{liu_empirical_2012,
	title = {Empirical evaluation of scoring functions for Bayesian network model selection},
	volume = {13},
	issn = {1471-2105},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3439716/},
	doi = {10.1186/1471-2105-13-S15-S14},
	abstract = {In this work, we empirically evaluate the capability of various scoring functions of Bayesian networks for recovering true underlying structures. Similar investigations have been carried out before, but they typically relied on approximate learning algorithms to learn the network structures. The suboptimal structures found by the approximation methods have unknown quality and may affect the reliability of their conclusions. Our study uses an optimal algorithm to learn Bayesian network structures from datasets generated from a set of gold standard Bayesian networks. Because all optimal algorithms always learn equivalent networks, this ensures that only the choice of scoring function affects the learned networks. Another shortcoming of the previous studies stems from their use of random synthetic networks as test cases. There is no guarantee that these networks reflect real-world data. We use real-world data to generate our gold-standard structures, so our experimental design more closely approximates real-world situations. A major finding of our study suggests that, in contrast to results reported by several prior works, the Minimum Description Length ({MDL}) (or equivalently, Bayesian information criterion ({BIC})) consistently outperforms other scoring functions such as Akaike's information criterion ({AIC}), Bayesian Dirichlet equivalence score ({BDeu}), and factorized normalized maximum likelihood ({fNML}) in recovering the underlying Bayesian network structures. We believe this finding is a result of using both datasets generated from real-world applications rather than from random processes used in previous studies and learning algorithms to select high-scoring structures rather than selecting random models. Other findings of our study support existing work, e.g., large sample sizes result in learning structures closer to the true underlying structure; the {BDeu} score is sensitive to the parameter settings; and the {fNML} performs pretty well on small datasets. We also tested a greedy hill climbing algorithm and observed similar results as the optimal algorithm.},
	pages = {S14},
	issue = {Suppl 15},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Liu, Zhifa and Malone, Brandon and Yuan, Changhe},
	urldate = {2022-10-10},
	date = {2012-09-11},
	pmid = {23046392},
	pmcid = {PMC3439716},
	file = {PubMed Central Full Text PDF:/home/levantsi/Zotero/storage/2V3PR2DX/Liu et al. - 2012 - Empirical evaluation of scoring functions for Baye.pdf:application/pdf},
}

@article{gross_machine_2020,
	title = {Machine Learning for Work Disability Prevention: Introduction to the Special Series},
	volume = {30},
	issn = {1573-3688},
	url = {https://doi.org/10.1007/s10926-020-09910-1},
	doi = {10.1007/s10926-020-09910-1},
	shorttitle = {Machine Learning for Work Disability Prevention},
	abstract = {Rapid development in computer technology has led to sophisticated methods of analyzing large datasets with the aim of improving human decision making. Artificial Intelligence and Machine Learning ({ML}) approaches hold tremendous potential for solving complex real-world problems such as those faced by stakeholders attempting to prevent work disability. These techniques are especially appealing in work disability contexts that collect large amounts of data such as workers’ compensation settings, insurance companies, large corporations, and health care organizations, among others. However, the approaches require thorough evaluation to determine if they add value to traditional statistical approaches. In this special series of articles, we examine the role and value of {ML} in the field of work disability prevention and occupational rehabilitation.},
	pages = {303--307},
	number = {3},
	journaltitle = {Journal of Occupational Rehabilitation},
	shortjournal = {J Occup Rehabil},
	author = {Gross, Douglas P. and Steenstra, Ivan A. and Harrell, Frank E. and Bellinger, Colin and Zaïane, Osmar},
	urldate = {2021-09-27},
	date = {2020-09-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/levantsi/Zotero/storage/UN6BZSZ4/Gross et al. - 2020 - Machine Learning for Work Disability Prevention I.pdf:application/pdf},
}

@article{li_hybrid_2018,
	title = {A Hybrid Structure Learning Algorithm for Bayesian Network Using Experts’ Knowledge},
	volume = {20},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/20/8/620},
	doi = {10.3390/e20080620},
	abstract = {Bayesian network structure learning from data has been proved to be a {NP}-hard (Non-deterministic Polynomial-hard) problem. An effective method of improving the accuracy of Bayesian network structure is using experts’ knowledge instead of only using data. Some experts’ knowledge (named here explicit knowledge) can make the causal relationship between nodes in Bayesian Networks ({BN}) structure clear, while the others (named here vague knowledge) cannot. In the previous algorithms for {BN} structure learning, only the explicit knowledge was used, but the vague knowledge, which was ignored, is also valuable and often exists in the real world. Therefore we propose a new method of using more comprehensive experts’ knowledge based on hybrid structure learning algorithm, a kind of two-stage algorithm. Two types of experts’ knowledge are defined and incorporated into the hybrid algorithm. We formulate rules to generate better initial network structure and improve the scoring function. Furthermore, we take expert level difference and opinion conflict into account. Experimental results show that our proposed method can improve the structure learning performance.},
	pages = {620},
	number = {8},
	journaltitle = {Entropy},
	author = {Li, Hongru and Guo, Huiping},
	urldate = {2022-10-10},
	date = {2018-08},
	langid = {english},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Bayesian network, explicit knowledge, hybrid algorithm, structure learning, vague knowledge},
	file = {Full Text PDF:/home/levantsi/Zotero/storage/XY8KICU7/Li and Guo - 2018 - A Hybrid Structure Learning Algorithm for Bayesian.pdf:application/pdf;Snapshot:/home/levantsi/Zotero/storage/NRQBW3YU/htm.html:text/html},
}


@article{schwarz_estimating_1978,
	title = {Estimating the Dimension of a Model},
	volume = {6},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full},
	doi = {10.1214/aos/1176344136},
	abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
	pages = {461--464},
	number = {2},
	journaltitle = {The Annals of Statistics},
	author = {Schwarz, Gideon},
	urldate = {2022-10-10},
	date = {1978-03},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62F99, 62J99, Akaike information criterion, asymptotics, dimension},
	file = {Full Text PDF:/home/levantsi/Zotero/storage/GHNCI3QI/Schwarz - 1978 - Estimating the Dimension of a Model.pdf:application/pdf;Snapshot:/home/levantsi/Zotero/storage/6TLDSZSB/1176344136.html:text/html},
}


@article{bartlett_integer_2017,
	title = {Integer Linear Programming for the Bayesian network structure learning problem},
	volume = {244},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370215000417},
	doi = {10.1016/j.artint.2015.03.003},
	series = {Combining Constraint Solving with Mining and Learning},
	abstract = {Bayesian networks are a commonly used method of representing conditional probability relationships between a set of variables in the form of a directed acyclic graph ({DAG}). Determination of the {DAG} which best explains observed data is an {NP}-hard problem [1]. This problem can be stated as a constrained optimisation problem using Integer Linear Programming ({ILP}). This paper explores how the performance of {ILP}-based Bayesian network learning can be improved through {ILP} techniques and in particular through the addition of non-essential, implied constraints. There are exponentially many such constraints that can be added to the problem. This paper explores how these constraints may best be generated and added as needed. The results show that using these constraints in the best discovered configuration can lead to a significant improvement in performance and show significant improvement in speed using a state-of-the-art Bayesian network structure learner.},
	pages = {258--271},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Bartlett, Mark and Cussens, James},
	urldate = {2022-10-13},
	date = {2017-03-01},
	langid = {english},
	keywords = {Bayesian networks, Constrained optimisation, Cutting planes, Integer Linear Programming, Separation},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\LEVANSI\\Zotero\\storage\\PU3NG4ZA\\Bartlett and Cussens - 2017 - Integer Linear Programming for the Bayesian networ.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\LEVANSI\\Zotero\\storage\\5XPF65VH\\S0004370215000417.html:text/html},
}

@article{rissanen_stochastic_1987,
	title = {Stochastic Complexity},
	volume = {49},
	issn = {2517-6161},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1987.tb01694.x},
	doi = {10.1111/j.2517-6161.1987.tb01694.x},
	abstract = {It is argued that all the useful information in observed data that can be extracted with a selected class of modeled distributions, will be obtained if we calculate the stochastic complexity, defined to be the shortest description length of the data. The same quantity also determines the greatest lower bound for prediction errors when the data are sequentially predicted. An abstract definition of stochastic complexity is given along with two fundamental theorems which justify the notion. Further, three explicit model selection criteria to approximate the stochastic complexity are described and the associated optimal models are interpreted to define asymptotically sufficient statistics for the data.},
	pages = {223--239},
	number = {3},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Rissanen, Jorma},
	urldate = {2022-10-16},
	date = {1987},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1987.tb01694.x},
	keywords = {minimum code length, model selection criteria, prior knowledge},
	file = {Full Text PDF:/home/levantsi/Zotero/storage/BB5XZB43/Rissanen - 1987 - Stochastic Complexity.pdf:application/pdf;Snapshot:/home/levantsi/Zotero/storage/USA7ZW9S/j.2517-6161.1987.tb01694.html:text/html},
}

@article{friedman_greedy_2001,
	title = {Greedy function approximation: A gradient boosting machine.},
	volume = {29},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full},
	doi = {10.1214/aos/1013203451},
	shorttitle = {Greedy function approximation},
	number = {5},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Friedman, Jerome H.},
	urldate = {2022-10-16},
	date = {2001-10-01},
	file = {Full Text:/home/levantsi/Zotero/storage/9Q84642D/Friedman - 2001 - Greedy function approximation A gradient boosting.pdf:application/pdf},
}


@inproceedings{merrick_explanation_2020,
	location = {Cham},
	title = {The Explanation Game: Explaining Machine Learning Models Using Shapley Values},
	isbn = {978-3-030-57321-8},
	doi = {10.1007/978-3-030-57321-8_2},
	series = {Lecture Notes in Computer Science},
	shorttitle = {The Explanation Game},
	abstract = {A number of techniques have been proposed to explain a machine learning model’s prediction by attributing it to the corresponding input features. Popular among these are techniques that apply the Shapley value method from cooperative game theory. While existing papers focus on the axiomatic motivation of Shapley values, and efficient techniques for computing them, they offer little justification for the game formulations used, and do not address the uncertainty implicit in their methods’ outputs. For instance, the popular {SHAP} algorithm’s formulation may give substantial attributions to features that play no role in the model. In this work, we illustrate how subtle differences in the underlying game formulations of existing methods can cause large differences in the attributions for a prediction. We then present a general game formulation that unifies existing methods, and enables straightforward confidence intervals on their attributions. Furthermore, it allows us to interpret the attributions as contrastive explanations of an input relative to a distribution of reference inputs. We tie this idea to classic research in cognitive psychology on contrastive explanations, and propose a conceptual framework for generating and interpreting explanations for {ML} models, called formulate, approximate, explain ({FAE}). We apply this framework to explain black-box models trained on two {UCI} datasets and a Lending Club dataset.},
	pages = {17--38},
	booktitle = {Machine Learning and Knowledge Extraction},
	publisher = {Springer International Publishing},
	author = {Merrick, Luke and Taly, Ankur},
	editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
	date = {2020},
	langid = {english},
	file = {Full Text PDF:/home/levantsi/Zotero/storage/4GMTLJZJ/Merrick and Taly - 2020 - The Explanation Game Explaining Machine Learning .pdf:application/pdf},
}


@article{airaksinen_development_2017,
	title = {Development and validation of a risk prediction model for work disability: Multicohort study},
	volume = {7},
	doi = {10.1038/s41598-017-13892-1},
	shorttitle = {Development and validation of a risk prediction model for work disability},
	abstract = {Work disability affects quality of life, earnings, and opportunities to contribute to society. Work characteristics, lifestyle and sociodemographic factors have been associated with the risk of work disability, but few multifactorial algorithms exist to identify individuals at risk of future work disability. We developed and validated a parsimonious multifactorial score for the prediction of work disability using individual-level data from 65,775 public-sector employees (development cohort) and 13,527 employed adults from a general population sample (validation cohort), both linked to records of work disability. Candidate predictors for work disability included sociodemographic (3 items), health status and lifestyle (38 items), and work-related (43 items) variables. A parsimonious model, explaining {\textgreater} 99\% of the variance of the full model, comprised 8 predictors: age, self-rated health, number of sickness absences in previous year, socioeconomic position, chronic illnesses, sleep problems, body mass index, and smoking. Discriminative ability of a score including these predictors was high: C-index 0.84 in the development and 0.83 in the validation cohort. The corresponding C-indices for a score constructed from work-related predictors (age, sex, socioeconomic position, job strain) were 0.79 and 0.78, respectively. It is possible to identify reliably individuals at high risk of work disability by using a rapidly-administered prediction score.},
	journaltitle = {Scientific Reports},
	shortjournal = {Scientific Reports},
	author = {Airaksinen, Jaakko and Jokela, Markus and Virtanen, Marianna and Oksanen, Tuula and Pentti, Jaana and Vahtera, Jussi and Koskenvuo, Markku and Kawachi, Ichiro and Batty, G. and Kivimäki, Mika},
	date = {2017-10-19},
	file = {Full Text PDF:/home/levantsi/Zotero/storage/8H628C7I/Airaksinen et al. - 2017 - Development and validation of a risk prediction mo.pdf:application/pdf},
}

